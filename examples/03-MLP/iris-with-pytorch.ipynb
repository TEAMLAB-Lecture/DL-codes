{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PyTorch Logistic Regression for Digits Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries",
        "import torch",
        "import torch.nn as nn",
        "import torch.optim as optim",
        "from torch.utils.data import DataLoader, TensorDataset",
        "",
        "from sklearn import datasets",
        "from sklearn.model_selection import train_test_split",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score",
        "import numpy as np",
        "import matplotlib.pyplot as plt",
        "import itertools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameters",
        "LEARNING_RATE = 0.01",
        "BATCH_SIZE = 64",
        "EPOCHS = 50 # You might need more epochs for better convergence",
        "",
        "# Device Configuration (GPU if available, else CPU)",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')",
        "print(f\"Using device: {device}\")",
        "",
        "# Set random seed for reproducibility",
        "SEED = 42",
        "np.random.seed(SEED)",
        "torch.manual_seed(SEED)",
        "if torch.cuda.is_available():",
        "    torch.cuda.manual_seed_all(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Loading and Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the digits dataset",
        "print(\"Loading digits dataset...\")",
        "digit_dataset = datasets.load_digits()",
        "print(\"Dataset loaded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assign features (X) and target (y) from dataset",
        "X_np = digit_dataset.data",
        "y_np = digit_dataset.target",
        "",
        "# Normalize the features (pixel values 0-16 -> 0-1)",
        "X_norm_np = X_np / 16.0",
        "",
        "# Split data into training and testing sets",
        "print(\"Splitting data into train/test sets...\")",
        "X_train_np, X_test_np, y_train_np, y_test_np = train_test_split(",
        "    X_norm_np, y_np, test_size=0.25, random_state=SEED, stratify=y_np # Use normalized X, Stratify for balanced classes",
        ")",
        "print(f\"Data split: X_train: {X_train_np.shape}, X_test: {X_test_np.shape}, y_train: {y_train_np.shape}, y_test: {y_test_np.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert NumPy arrays to PyTorch tensors",
        "X_train = torch.tensor(X_train_np, dtype=torch.float32)",
        "y_train = torch.tensor(y_train_np, dtype=torch.long) # CrossEntropyLoss expects Long type for labels",
        "X_test = torch.tensor(X_test_np, dtype=torch.float32)",
        "y_test = torch.tensor(y_test_np, dtype=torch.long)",
        "",
        "# Create TensorDatasets and DataLoaders",
        "train_dataset = TensorDataset(X_train, y_train)",
        "test_dataset = TensorDataset(X_test, y_test)",
        "",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Logistic Regression is equivalent to a single linear layer in a neural network",
        "# Input features = 64 (8x8 image flattened)",
        "# Output features = 10 (number of classes, digits 0-9)",
        "class PyTorchLogisticRegression(nn.Module):",
        "    def __init__(self, input_dim, output_dim):",
        "        super(PyTorchLogisticRegression, self).__init__()",
        "        self.linear = nn.Linear(input_dim, output_dim)",
        "",
        "    def forward(self, x):",
        "        # No activation function here, as CrossEntropyLoss applies LogSoftmax internally",
        "        outputs = self.linear(x)",
        "        return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Instantiate Model, Loss, Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_dim = X_train.shape[1] # Should be 64",
        "output_dim = len(digit_dataset.target_names) # Should be 10",
        "",
        "model = PyTorchLogisticRegression(input_dim, output_dim).to(device)",
        "print(\"\\nModel Architecture:\")",
        "print(model)",
        "",
        "# CrossEntropyLoss combines LogSoftmax and NLLLoss - suitable for multi-class classification",
        "criterion = nn.CrossEntropyLoss()",
        "",
        "# Optimizer (Adam is a popular choice)",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)",
        "# You could also use SGD:",
        "# optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"\\nStarting Training for {EPOCHS} epochs...\")",
        "for epoch in range(EPOCHS):",
        "    model.train() # Set model to training mode",
        "    running_loss = 0.0",
        "    for i, (features, labels) in enumerate(train_loader):",
        "        # Move tensors to the configured device",
        "        features = features.to(device)",
        "        labels = labels.to(device)",
        "",
        "        # Forward pass",
        "        outputs = model(features)",
        "        loss = criterion(outputs, labels)",
        "",
        "        # Backward pass and optimize",
        "        optimizer.zero_grad() # Clear previous gradients",
        "        loss.backward()       # Compute gradients",
        "        optimizer.step()        # Update weights",
        "",
        "        running_loss += loss.item()",
        "",
        "    # Print average loss for the epoch",
        "    epoch_loss = running_loss / len(train_loader)",
        "    if (epoch + 1) % 5 == 0 or epoch == 0: # Print every 5 epochs or the first epoch",
        "      print(f\"Epoch [{epoch+1}/{EPOCHS}], Loss: {epoch_loss:.4f}\")",
        "",
        "print(\"Training finished.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nEvaluating model on the test set...\")",
        "model.eval() # Set model to evaluation mode",
        "all_preds = []",
        "all_labels = []",
        "",
        "# Disable gradient calculations during evaluation",
        "with torch.no_grad():",
        "    correct = 0",
        "    total = 0",
        "    for features, labels in test_loader:",
        "        features = features.to(device)",
        "        labels = labels.to(device)",
        "",
        "        outputs = model(features)",
        "",
        "        # Get predictions from the maximum value output (logits)",
        "        # The output is (batch_size, num_classes)",
        "        _, predicted = torch.max(outputs.data, 1)",
        "",
        "        total += labels.size(0)",
        "        correct += (predicted == labels).sum().item()",
        "",
        "        # Store predictions and labels for detailed metrics",
        "        all_preds.extend(predicted.cpu().numpy())",
        "        all_labels.extend(labels.cpu().numpy())",
        "",
        "accuracy = 100 * correct / total",
        "print(f\"\\nTest Accuracy: {accuracy:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Detailed Metrics (Confusion Matrix, Classification Report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n--- Detailed Test Set Metrics ---\")",
        "print(\"\\nConfusion Matrix:\")",
        "cm = confusion_matrix(all_labels, all_preds)",
        "print(cm)",
        "",
        "print(\"\\nClassification Report:\")",
        "print(classification_report(all_labels, all_preds, target_names=[str(i) for i in digit_dataset.target_names]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Confusion Matrix Plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the plotting function (from scikit-learn examples)",
        "def plot_confusion_matrix(cm, classes,",
        "                          normalize=False,",
        "                          title='Confusion matrix',",
        "                          cmap=plt.cm.Blues):",
        "    \"\"\"",
        "    This function prints and plots the confusion matrix.",
        "    Normalization can be applied by setting `normalize=True`.",
        "    \"\"\"",
        "    if normalize:",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]",
        "        print(\"Normalized confusion matrix\")",
        "    else:",
        "        print('Confusion matrix, without normalization')",
        "",
        "    # print(cm) # Optionally print matrix values here",
        "",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)",
        "    plt.title(title)",
        "    plt.colorbar()",
        "    tick_marks = np.arange(len(classes))",
        "    plt.xticks(tick_marks, classes, rotation=45)",
        "    plt.yticks(tick_marks, classes)",
        "",
        "    fmt = '.2f' if normalize else 'd'",
        "    thresh = cm.max() / 2.",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):",
        "        plt.text(j, i, format(cm[i, j], fmt),",
        "                 horizontalalignment=\"center\",",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")",
        "",
        "    plt.tight_layout()",
        "    plt.ylabel('True label')",
        "    plt.xlabel('Predicted label')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nPlotting Confusion Matrix...\")",
        "class_names = [str(i) for i in digit_dataset.target_names]",
        "",
        "# Plot non-normalized confusion matrix",
        "plt.figure(figsize=(8, 6))",
        "plot_confusion_matrix(cm, classes=class_names,",
        "                      title='PyTorch Confusion Matrix (Digits), without normalization')",
        "plt.show() # Show plot immediately after creation",
        "",
        "# Plot normalized confusion matrix",
        "plt.figure(figsize=(8, 6))",
        "plot_confusion_matrix(cm, classes=class_names, normalize=True,",
        "                      title='PyTorch Normalized Confusion Matrix (Digits)')",
        "plt.show() # Show plot immediately after creation",
        "",
        "",
        "print(\"\\nScript finished.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}