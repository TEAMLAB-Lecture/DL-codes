{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np",
        "from sklearn.datasets import load_iris",
        "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler",
        "# Note: The original input mentioned 'import sklearn' separately,",
        "# but specific modules are imported, so this covers the needs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the Iris dataset",
        "datasets = load_iris()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract feature data and show the first 5 rows",
        "x_data = datasets[\"data\"]",
        "print(\"First 5 rows of x_data:\")",
        "print(x_data[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract target data",
        "y_data = datasets[\"target\"]",
        "print(\"\\nOriginal y_data:\")",
        "print(y_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reshape y_data to be a column vector",
        "y_data = y_data.reshape([-1,1])",
        "print(\"\\nFirst 3 rows of reshaped y_data:\")",
        "print(y_data[:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# One-hot encode the target data",
        "enc = OneHotEncoder()",
        "enc.fit(y_data)",
        "y_data = enc.transform(y_data).toarray()",
        "print(\"\\nLast row of one-hot encoded y_data:\")",
        "print(y_data[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scale the feature data using MinMaxScaler",
        "min_max_scaler = MinMaxScaler()",
        "x_data_minmax = min_max_scaler.fit_transform(x_data)",
        "print(\"\\nFirst 3 rows of scaled x_data:\")",
        "print(x_data_minmax[:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add a bias term (column of ones) to the feature data",
        "x_0 = np.ones(x_data_minmax.shape[0])",
        "x_data_minmax = np.column_stack((x_0, x_data_minmax))",
        "print(\"\\nFirst 3 rows of x_data with bias term:\")",
        "print(x_data_minmax[:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize weights randomly",
        "# Note: Original execution order was different (In[171] after In[175])",
        "# Placing it here for logical flow before use.",
        "weights = np.random.uniform(size=(3,5)) # 3 classes, 5 features (4 + bias)",
        "print(\"\\nInitial random weights:\")",
        "print(weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the softmax function",
        "def softmax(z):",
        "    # Subtract max for numerical stability",
        "    z_stable = z - np.max(z, axis=1, keepdims=True)",
        "    e = np.exp(z_stable)",
        "    p = e / np.sum(e, axis=1, keepdims=True)",
        "    return p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate the linear combination (z)",
        "z = x_data_minmax.dot(weights.T)",
        "print(\"\\nShape of z:\")",
        "print(z.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate softmax for the first 5 samples",
        "print(\"\\nSoftmax output for first 5 samples:\")",
        "print(softmax(z[:5]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the cross-entropy loss function",
        "# Note: Original execution order was different (In[107] after In[191])",
        "# Defining functions before they are called.",
        "def cross_entropy_function(y, x, weights):",
        "    m = y.shape[0]",
        "    z = x.dot(weights.T)",
        "    p = softmax(z)",
        "    # Adding a small epsilon to log(p) to avoid log(0)",
        "    epsilon = 1e-7",
        "    log_likelihood = -np.sum(y * np.log(p + epsilon))",
        "    cost = log_likelihood / m",
        "    return cost # Returning average cost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate the initial cross-entropy cost",
        "# Note: Using the modified cross_entropy_function returning average cost",
        "initial_cost = cross_entropy_function(y_data, x_data_minmax, weights)",
        "print(f\"\\nInitial average cross-entropy cost: {initial_cost}\")",
        "# The original output 173.45 was likely the total sum, not the average.",
        "# Total sum would be initial_cost * 150"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the gradient descent minimization function",
        "def minimize_grdient(y, x, initial_weights, iterations = 50000, alpha=0.1): # Adjusted iterations/alpha",
        "    cost_history= []",
        "    theta_history = []",
        "    m = y.shape[0] # Number of samples (150)",
        "    theta = np.copy(initial_weights) # Shape (3, 5)",
        "",
        "    print(\"\\nStarting Gradient Descent Training...\")",
        "    for i in range(iterations):",
        "        z = x.dot(theta.T) # Shape (150, 3)",
        "        p = softmax(z)     # Shape (150, 3)",
        "        error = p - y      # Shape (150, 3)",
        "",
        "        gradient = (x.T.dot(error)).T / m # Shape (3, 5)",
        "",
        "        theta = theta - alpha * gradient",
        "",
        "        cost = cross_entropy_function(y, x, theta)",
        "        cost_history.append(cost)",
        "        #theta_history.append(np.copy(theta)) # Optional: store weights history",
        "",
        "        if (i % 5000) == 0: # Print cost every 5000 iterations",
        "            print(f\"Iteration {i:5d}, Average Cost: {cost:.6f}\")",
        "",
        "    print(\"Training finished.\")",
        "    return theta, cost_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the gradient descent training",
        "# Note: The original had 500k iterations and alpha 0.001.",
        "# Adjusted for potentially faster convergence/demonstration.",
        "# You might need to tune iterations and alpha.",
        "# The commented line from the original input:",
        "# weights = minimize_grdient(y_data, x_data_minmax,weights)",
        "theta, cost_history = minimize_grdient(y_data, x_data_minmax, weights, iterations=50000, alpha=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select random indices for testing",
        "np.random.seed(42) # for reproducibility",
        "rand_index= np.random.randint(0, x_data_minmax.shape[0], 30)",
        "print(\"\\nRandom indices for testing:\")",
        "print(rand_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make predictions on the selected samples",
        "z_pred = x_data_minmax[rand_index].dot(theta.T)",
        "y_pred_probs = softmax(z_pred)",
        "y_pred = np.argmax(y_pred_probs, axis=1)",
        "print(\"\\nPredicted labels for random samples:\")",
        "print(y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the true labels for the selected samples",
        "y_true = np.argmax(y_data[rand_index], axis=1)",
        "print(\"\\nTrue labels for random samples:\")",
        "print(y_true)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare predictions with true labels",
        "comparison = (y_pred == y_true)",
        "print(\"\\nComparison (Prediction == True):\")",
        "print(comparison)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate accuracy on the selected samples",
        "accuracy = np.sum(comparison) / len(rand_index)",
        "print(f\"\\nAccuracy on random samples: {accuracy:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}